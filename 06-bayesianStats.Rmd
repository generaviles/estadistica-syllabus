#Bayesian Statistics


##Math for understanding the Bayesian approach

$$P(A_k|B)=\frac{P(A_k\cap B)}{P(B)}$$

$$P(B|A_k)=\frac{P(B\cap A_k)}{P(A_k)} \therefore P(A_k|B)=\frac{}{}$$

###Ejercicio
Morgan asks Smith to pick a number between 20 and 50, Smith tells Morgan a characteristic of the number he picked (it is a multiple of 5). With this information, determine the probability of Smith's pick being also a multiple of 3 using ven diagrams and conditional probability.

Población total de 1000, una enfermedad se presenta en la población con prevalencia del 10%. Para poder 

##Conceptos sueltos que debo cubrir

  - Tamaño Muestral Equivalente.
  - Ajuste de Laplace.
    - Laplace sugiere sumar un 1 ($\frac{n_1+1}{n_1+n_0+2}$)

$$\frac{n_1+\alpha}{n_1+n_0+(\alpha + \beta)}$$

  - Redes bayesianas, escribir sobre ésto.

$$p(p|D) \sim \beta (\alpha + n_1, \beta +n_0)$$

$$p(p) \sim \beta(\alpha, \beta)$$

  - Regresión Bayesiana.

##Entropy and Information Gain

###Key definitios

**Formula for Entropy**

$$H(x) = -\sum_{i=1}^{n}P(x_i)*log_2P(x_i)$$

**Information Gain** tells us how important a given attribute of the feature vectors is for predicting that characteristic.
Mathematically it is expressed as follows:

$$Gain(x,y) = H(x)-\sum_{v\in Values(A_i)}P(A_i=v)H(S_v)$$

###Examples

####Example 1

```{r}
#Creating matrix
test<-matrix(c(346,74,124,26,470,100), nrow = 3, ncol = 2, byrow = TRUE)
colnames(test) <- c("count", "%")
rownames(test) <- c("Yes", "No", "Overall")

test
```

Following the formula for entropy shown in the previous section we can obtain the **entropy** value for this dataset:

$$H(dataset) = -[\{P(yes)*log_2P(yes)+P(no)*log_2P(no)\}] = [(0.74*log_2(0.74))+(0.26*log_2(0.26))]$$

This operation gives the following result:
```{r}
#Entropy for the node
H <- -((0.74*log(0.74, base = 2))+(0.26*log(0.26, base = 2)))
H
```

We can, therefore, conclude that the entropy in this node is **0.827**

####Example 2. Usig Entropy and Information Gain to classify

We will use a very simple dataset:
```{r}
test2 <- matrix(c(1,1,1,"I",1,1,0,"I",0,0,1,"II",1,0,0,"II"), ncol = 4, nrow = 4, byrow = TRUE)
colnames(test2) <- c("X", "Y", "Z", "Class")
test2 <- as.data.frame(test2)
test2
```

The challenge is presented to use only one attribute to predict which `Class` each value belongs to. The approximation is done following these steps:

  1. Calculate entropy values for each attribute.
  2. Calculate information gain.
  3. Select the attribute with higher information gain result.
  
```{r}
X1 <- table(test2$X == 1)
X0 <- table(test2$X == 0)
data.frame(X1, X0)
```


**Step 1**

  - Entropy for attribute `X`
  

$$H(X_1) = -\left[ \frac{2}{3}*log_2\frac{2}{3} + \frac{1}{3}*log_2\frac{1}{3} \right] = 0.9183 $$
```{r}
#X1 <- -((2/3*log(2/3, base = 2))+(1/3*log(1/3, base = 2)))
#X0 <- -((1/1*log(1/1, base = 2)))
#Hx <- 
```

  - Information Gain for attribute `X`

$$I = 1-P(Y=1)*H(Y=1)-P(X=0)*H(X=0)=1-\left( \frac{3}{4}*0.9183\right)-\left(\frac{1}{4}*0\right)=1-0.688725-0$$
$$= \boldsymbol{0.311275}$$

**Step 2**

  - Entropy for attribute `Y`

$$H(X_1) = -\left[ \frac{2}{2}*log_2\frac{2}{2} \right] = 0 $$


  - Information Gain for attribute `Y`

$$I = 1-P(Y=1)*H(Y=1)-P(Y=0)*H(Y=0)=1- 0-0= \boldsymbol{1}$$

**Step 3**

  - Entropy for attribute `Z`

$$H(Z_1) = -\left[ \frac{1}{2}*log_2\frac{1}{2}\right] = 0.5 $$


  - Information Gain for attribute `Z`

$$I = 1-P(Z=1)*H(Z=1)-P(Z=0)*H(Z=0)=1- 0.5-0.5= \boldsymbol{0}$$

According to the results these are the corresponding **information gain** results for each attribute:

  - `X`: 0.311275
  - `Y`: 1
  - `Z`: 0

We can then safely state that using attribute `Y` to predict the target variable `Class` is the best option based on information gain.


##Bayesian Linear Regression

Before exploring the Bayesian approach to regressions, it is important to refresh our memory and review the traditional approach to regressions.


###Linear Regression

For this we will use the example of the *cereals* data set [@cereals].
```{r, message=FALSE, warning=FALSE}
library(readr)
cereals <- read_delim("D:/Dropbox/MsC UABC/2o Semestre/Clases/Estadistica/estadistica-syllabus/datasets/cereals.csv", 
    "\t", escape_double = FALSE, trim_ws = TRUE)
str(cereals)
```

####Data Preparation

As we can see, this dataset needs som data preparation before a linear regression model can be generated.
```{r}
#Converting to numeric data those variables that need to be read as such>

cereals$calories <- as.numeric(cereals$calories)
cereals$protein <- as.numeric(cereals$protein)
cereals$fat <- as.numeric(cereals$fat)
cereals$sodium <- as.numeric(cereals$sodium)
cereals$fiber <- as.numeric(cereals$fiber)
cereals$carbo <- as.numeric(cereals$carbo)
cereals$sugars <- as.numeric(cereals$sugars)
cereals$potass <- as.numeric(cereals$potass)
cereals$vitamins <- as.character(cereals$vitamins)
cereals$weight <- as.numeric(cereals$weight)
cereals$cups <- as.numeric(cereals$cups)
cereals$rating <- as.numeric(cereals$rating)

str(cereals)
```

####Data Quality Report for Categorical Variables

```{r}
setwd("D:/Dropbox/MsC UABC/2o Semestre/Clases/Estadistica/estadistica-syllabus")
source('QOfCategoricalF.R')

DQRcat <- QOfCategoricalF(cereals[,c(1,2,3,12,13)])

DQRcat
```


####Data Quality Report for Continuous Variables

```{r}
source('QOfContinuousF.R')

DQRcont <- QOfContinuousF(cereals[,c(4:11, 14:16)])

DQRcont

```

###Logistic Regression

####Splitting train and test dataset

```{r}
#Spliting inot trainning and test datasets

library(caTools)
set.seed(123)
split = sample.split(cereals$rating, SplitRatio = 2/3)
training_set = subset(cereals, split == TRUE)
test_set = subset(cereals, split == FALSE)

# Feature Scaling
#training_set[, c(4:11,14:16)] = scale(training_set[, c(4:11,14:16)])
# test_set[, 2:3] = scale(test_set[, 2:3])

```

####Fitting SLR to training set

```{r}
regressor <- lm(formula = rating ~ sugars,
                data = cereals)

summary(regressor)
```

####Visualizing SLR on the copmlete `cereals` dataset

```{r, fig.align= 'center'}
library(ggplot2)
ggplot() +
  geom_point(aes(x = cereals$sugars, y = cereals$rating),
             colour = "red") +
  geom_line(aes(x = cereals$sugars, y = predict(regressor, newdata = cereals)),
            colour = "blue") +
            ggtitle("Sugar vs Prediction of Rating variables") +
            theme(plot.title = element_text(hjust = 0.5)) +
            xlab("Amount of Sugars") +
            ylab("Raiting")
```

## Examples of Bayesian models

Taken from [Rasmus Bååth´s blog.](http://sumsar.net/blog/2017/02/introduction-to-bayesian-data-analysis-part-one/)

### Bayesian A testing for Swedish Fish Incorporated

Swedish Fish Incorporated is the largest Swedish company delivering fish by mail order. They are now trying to get into the lucrative Danish market by selling one year Salmon subscriptions. The marketing department have done a pilot study and tried the following marketing method:

**A**: Sending a mail with a colorful brochure that invites people to sign up for a one year salmon subscription.

The marketing department sent out 16 mails of type A. Six Danes that received a mail signed up for one year of salmon and marketing now wants to know, how good is method A?

#### Question I: Build a Bayesian model that answers the question: What would the rate of sign-up be if method A was used on a larger number of people?

**Hint 1**: The answer is not a single number but a distribution over probable rates of sign-up.

**Hint 2**: As part of you generative model you’ll want to use the binomial distribution, which you can sample from in R using the  rbinom(n, size, prob). The binomial distribution simulates the following process n times: The number of “successes” when performing size trials, where the probability of “success” is prob.

**Hint 3**: A commonly used prior for the unknown probability of success in a binomial distribution is a uniform distribution from 0 to 1. You can draw from this distribution by running runif(1, min = 0, max = 1)

**Hint 4**: Here is a code scaffold that you can build upon.

  - First of all we need to declare the amount of draws our model will make and the *prior* distribution.
  
```{r}
# Number of random draws from the prior
set.seed(123)
n_draw <- 100000

prior <- runif(n_draw,0,1) # Here you sample n_draws draws from the prior  
hist(prior,
     col = "pink",
     main = "Hisogram of Prior Distribution for Strategy A") # It's always good to eyeball the prior to make sure it looks ok.
```

  - Once this is finished we declare a generative model and simulate data with the parameters from the priors and the model.

```{r}
# Here you define the generative model
generative_model <- function(rate) {
  subscribers <- rbinom(1, size = 16, prob = rate)
  subscribers
}

# Here you simulate data using the parameters from the prior and the 
# generative model
subscribers <- rep(NA, n_draw)
for(i in 1:n_draw) {
  subscribers[i] <- generative_model(prior[i])
}
```

  - Next we work on filtering the draws that mached the data and generate a histogram of it, this will be the visualization of the **Posterior Distribution**.

```{r}

# Here you filter off all draws that do not match the data.
posteriorA <- prior[subscribers == 6] 

hist(posteriorA,
     col = "coral2",
     xlim = c(0,1)) # Eyeball the posterior
length(posteriorA) # See that we got enought draws left after the filtering.
# There are no rules here, but you probably want to aim
# for >1000 draws.

# Now you can summarize the posterior, where a common summary is to take the mean
# or the median posterior, and perhaps a 95% quantile interval.
summary(posteriorA)
```

As we can see from the previous calculations, if strategy A was used in a larger population we could expect a sign up rate from **20%** to **60%**, this result shows a considerable amount of **uncertanty**.

#### Question II: What’s the probability that method A is better than telemarketing?

So marketing just told us that the rate of sign-up would be 20% if salmon subscribers were snared by a telemarketing campaign instead (to us it’s very unclear where marketing got this very precise number from). So given the model and the data that we developed in the last question, what’s the probability that method A has a higher rate of sign-up than telemarketing?

**Hint 1**: If you have a vector of samples representing a probability distribution, which you should have from the last question, calculating the amount of probability above a certain value is done by simply counting the number of samples above that value and dividing by the total number of samples.

**Hint 2**: The answer to this question is a one-liner.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
sum(posteriorA > 0.2)/length(posteriorA)
```

As we can see, it is more likely that strategy **A** will produce higher sign up rattings than telemarketing.

#### Question III: If method A was used on 100 people what would be the number of sign-ups?

**Hint 1**: The answer is again not a single number but a distribution over probable numbers of sign-ups.

**Hint 2**: As before, the binomial distribution is a good candidate for how many people that sign up out of the 100 possible.

**Hint 3**: Make sure you don’t “throw away” uncertainty, for example by using a summary of the posterior distribution calculated in the first question. Use the full original posterior sample!

**Hint 4**: The general patter when calculating “derivatives” of posterior samples is to go through the values one-by-one, and perform a transformation (say, plugging in the value in a binomial distribution), and collect the new values in a vector.

```{r, fig.align='center'}

# But since rbinom is vectorized we can simply write it like this:
signupsA <- rbinom(n = length(posteriorA), size = 100, prob = posteriorA)

hist(signupsA, xlim = c(0, 100),
     col = "pink",
     main = "Histograms of Signups Projected in 100
     Potential Clients With Strategy A")

```

The probable number of signups if strategy A were to be used is between **20** and **60**.

### Calculating probabilies for a new marketing idea.

The CEO of Swedish Fish Incorporated came with the "greatest idea" for marketing: along with the colorful brochure he wants to send a frozen salmon to the potential costumers. He has learned on or two things from marketing and had his idea already tested with **16** danes, out of these **10** sigend up for the service. </br>
What's the probability of this strategy to generate signups in a larger population? </br>


  - First we need to generate the analysis for the new strategy:
    - A prior distribution to start from, we'll go with a uniform distribution:

```{r, fig.align='center'}
n_draw <- 100000

prior <- runif(n_draw,0,1) # Here you sample n_draws draws from the prior  
hist(prior,
     col = "dodgerblue",
     main = "Hisogram of Prior Distribution for Strategy B") # It's always good to eyeball the prior to make sure it looks ok.
```

    - The regenerative model and all the calculations will be done the same as strategy A, but instead of selecting those who resulted in 6, we will select those draws who resulted in 10.
    
    
```{r, echo=FALSE, message=FALSE, warning=FALSE}

generative_model <- function(rate) {
  subscribers <- rbinom(1, size = 16, prob = rate)
  subscribers
}

# Here you simulate data using the parameters from the prior and the 
# generative model
subscribers <- rep(NA, n_draw)
for(i in 1:n_draw) {
  subscribers[i] <- generative_model(prior[i])
}

# Here you filter off all draws that do not match the data.
posteriorB <- prior[subscribers == 10] 

histB <- hist(posteriorB,
     col = "dodgerblue4",
     xlim = c(0,1),
     main = "Histogram of Posterior Distribution for Strategy B") # Eyeball the posterior
histB

length(posteriorB) # See that we got enough draws left after the filtering.
# There are no rules here, but you probably want to aim
# for >1000 draws.

# Now you can summarize the posterior, where a common summary is to take the mean
# or the median posterior, and perhaps a 95% quantile interval.
summary(posteriorB)

```

  - It is time to compare these two aproximations:

```{r, include=FALSE}
signupsB <- rbinom(n = length(posteriorB), size = 100, prob = posteriorB)
```

```{r, fig.align='center'}
par(mfrow = c(2,1))
hist(signupsA, xlim = c(0, 100),
     col = "pink",
     main = "Histograms of Signups Projected in 100
     Potential Clients With Strategy A")
hist(signupsB, xlim = c(0, 100),
     col = "dodgerblue4",
     main = "Histograms of Signups Projected in 100
     Potential Clients With Strategy B")
```

  - It appears that Strategy **B** is more likely to yield higher signups. In order to check if it is so we will generate a table of rate differences:
```{r, include=FALSE}
rate <- data.frame(as.vector(posteriorA[1:5871]), as.vector(posteriorB))
colnames(rate) <- c("PostA", "PostB")
rate$rate_diff <- rate$PostB - rate$PostA
```

Where the variable `rate_diff` is calculated as follows, since we are interested in variable **B**:

$$rate\_diff = \text{rateB}-\text{rateA}$$

The next table is a representation of the first values obtained:

```{r, echo=FALSE}
head(rate)
```

A histogram of the values recorded in `rate_diff` is usefull to visualize the probability of strategy B yelding more signups than strategy A.
```{r, echo=FALSE}
hist(rate$rate_diff,
     col = "brown1",
     main = "Histogram of the Rate Difference")
```

It becomes obvious that the greater part of the probability is beyond 0, meaning that strategy B is more likely to bring mor signups to the company. </br>
If the CEO wanted to know how much likely the probability of strategy B is to be sucessful (more signups) than strategy A, this could be computated as well with the followin lines of code:
```{r}
sum((rate$rate_diff > 0)/length(rate$rate_diff))
```

**Strategy *B* is 91.4% more likely to bring more signups than strategy A.**
