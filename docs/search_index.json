[
["index.html", "Estadística para el Análisis de Datos Capítulo 1 Introducción", " Estadística para el Análisis de Datos Gener Avilés R 2017-08-16 Capítulo 1 Introducción Curso impartido por: Dr. Juan Iván Nieto Hipólito Dra. María de los Ángeles Cosío León "],
["conceptos-generales.html", "Capítulo 2 Conceptos Generales 2.1 Terms comparing statistics and computer sciences 2.2 Teoria de Conjuntos 2.3 Diagramas de Venn 2.4 Algebra de Conjuntos 2.5 Leyes de Morgan 2.6 Modelos De Probabilidad 2.7 Probabilidad Condicional 2.8 Modelos de Distribución Probabilística y variables aleatorias.", " Capítulo 2 Conceptos Generales 7 de Febrero 2017 Cuando se tiene un modelo conocido que se asemeja al conjunto de datos, se le llama modelo parametrico. P.ej: \\[\\int_{a}^{b} f(x)dx\\] La robustez del modelo la da el coeficiente de correlacion (\\(R^2\\)), el cual nos habla de que tanto se parece el modelo a los datos. los valores van de \\(0-1\\), un valor \\(&gt;0.9\\) se considera adecuado. Cunado los datos no se parecen a un modelo ya conocido, se utilizan los modelos no parametricos. 2.1 Terms comparing statistics and computer sciences 2.2 Teoria de Conjuntos 2.2.1 Definiciones Conjunto: Una coleccion de elementos con caracteristicas que comparten entre ellos. \\[A=\\text{{1,2,3,4}}\\] \\[B = \\text{{numeros enteros positivos}}\\] \\[ C = \\text{{numeros enteros &gt; 3}} = \\text{{4,5,6...}}\\] Todos estos tipos de conjuntos se pueden convertir eventualmente en el espacio muestral, por lo tanto la definicion del espacio muestral es vital antes de iniciar procesos de anlalisis. Alternativamente, los elementos \\(x\\) de un conjunto se pueden enunciar de tal manera que cumplan una caracteristica \\(P\\). \\[\\text{{x|x satisface P}}\\] En esta notacion, el simbolo \\(|\\) se interpreta como: “tal que”. Otro ejemplo con esta notacion: \\[ E = \\{x|x \\space numero\\space entero\\space 4 \\leq x \\leq 10\\}\\] Pertenece a (\\(\\in\\)): Tomando el ejemplo anterior, podemos decir que \\(x = 5 \\in E\\). No pertenece a (\\(\\notin\\)) Tomando el ejemplo anterior podemos decir que \\(x = 11 \\notin E\\) Subconjunto (\\(\\subset\\)) Cuando todos los elementos del conjunto \\(A\\) estan incluidos en el conjunto \\(B\\) (\\(A \\subset B\\)) Complemento (\\(^c\\)) El complemento de un conjunto \\(S\\), con respecto al conjunto universo (\\(\\Omega\\)), otros autores utilizan \\(U\\), es el conjunto de todos los elementos que no pertenecen a \\(S\\), y se denota como \\(S^c\\). Es importante recordar que \\(\\Omega^c = \\emptyset\\) donde \\(\\emptyset\\) es el conjunto vacio. Union (\\(\\cup\\)) \\[S \\cup T = \\{x|x \\in or\\space x\\in T\\}\\] Intereseccion (\\(\\cap\\)) \\[S \\cap T = \\{x|x \\in S\\space y \\space x \\in T \\} \\] Por lo tanto: \\[\\bigcup_{n=1}^{\\infty}= S_1 \\cup S_2 \\cup ...= \\{ x|x \\in S_n para \\space alguna\\space n\\} \\] Donde \\(para\\space S_n = \\space un \\space entero \\space positivo\\). \\[ \\bigcap_{n=1}^{\\infty} = S_1 \\cap S_2 \\cap ... = \\{ x|x \\in S_n \\space para \\space alguna \\space n\\}\\] 2.2.2 Conjuntos Disjuntos Son aquellos cuya interseccion (\\(\\cap\\)) es el conjunto vacio (\\(\\emptyset\\)). No se intersectan. \\[S \\cap T = \\{ \\emptyset\\}\\] La probabilidad del conjunto vacio no es igual a \\(\\emptyset\\), por lo tanto habra que calcularla en cada caso. 2.2.3 Partición: Una colección de conjuntos se dice que es una particion de un conjunto \\(S\\) si los conjuntos en la coleccion son disjuntos y su union es \\(S\\). \\[S = \\{1,2,3,4,5,6,7,8\\}\\] \\(A = \\{1,2,3\\}\\), \\(B = \\{4,5,6,7,8\\}\\) y \\(C = \\{\\emptyset\\}\\) Ojo: simpre considerar la probabilidad del conjunto vacio, siempre existe. Si \\(x\\) y \\(y\\) son 2 objetos \\((x,y)\\) para describir un par ordenado, por ejemplo, numeros reales \\(R\\), el conjunto de pares (o tripleta) de escalares se puede escribir como \\(R^2\\) y \\(R^3\\), respectivamente. 2.3 Diagramas de Venn \\(A \\cap B\\) \\(A\\cup B\\) \\(A^c \\cap B\\) \\(\\text{Conjuntos Disjuntos}\\) \\(S,T,\\space y \\space V \\space son \\space una \\space particion \\space de \\space \\Omega\\) 2.4 Algebra de Conjuntos Propiedad asociativa ???? \\[S \\cup T = T \\cup S\\] \\[S \\cap (T \\cup V) = (S \\cap T) \\cup (S \\cap V)\\] \\[(S^c)^c = S\\] \\[S \\cup \\Omega = \\Omega\\] Propiedad Distributiva???? \\[S \\cup (T \\cup V) = (S \\cup T)\\cup V\\] \\[S \\cup (T \\cap V) = (S \\cup T)\\cap (S \\cap V)\\] \\[S \\cap S^c = \\emptyset\\] \\[S \\cap \\Omega = S\\] 2.5 Leyes de Morgan Considerar las siguentes notaciones: \\((\\bigcup_{n}^{}S_n)^c = \\bigcap_{n}^{}{S_n}^c\\) \\((\\bigcap S_n)^2 = \\bigcup_{n}^{}{S_n}^c\\) *Notar como cambian de \\(\\cup\\) a \\(\\cap\\) y viceversa, en estas leyes. 8 de Febrero de 2017 2.6 Modelos De Probabilidad Buscan proveer certidumbre o reducir incertidumbre. 2.6.1 Elementos de un Modelo de Probabilidad El espacio muestral (\\(\\Omega\\)): Entendido como el conjunto de todos los posibles resultados de un experimento. Ley de probabilidad, la cual asigna a un conjunto \\(A\\) de posibles resultados (llamdo también evento) un número no-negativo (\\(P(A) = (\\text{la probabilidad de A})\\)) que codifica nuestro conocimiento o creencia sobre la probabilidad de los elementos de \\(A\\). 2.6.2 Axiomas de Probabilidad No negatividad Para cada uno de los eventos en \\(A\\): \\(P(A) \\geq 0\\) Adición Si \\(A\\) y \\(B\\) son dos eventos disjuntos, entonces la posibilidad de su unión satisface: \\(P(A \\cup B) = P(A)+P(B)\\) ó \\(P(A_1\\cup A_2 \\cup ...A_n) = P(A_1) + P(A_2)+...P(A_n)\\) Normalización La probabilidad del copmleto (entire) espacio muestral \\(\\Omega\\) es \\(1\\), esto es \\(P(\\Omega) = 1\\). Ojo: Los eventos del espacio muestral deben ser eventos mutuamente excluyentes, osea que no puedan suceder simultaneamente. 2.6.3 Ley de Probabilidad Discreta Probabilidad discreta: Asignar a un evento una probabilidad única (valor). Si el espacio de probabilidad consiste de un número finito de posibles resultados, entonces la ley de probabilidad se especifica por las probabilidades de cada uno de los eventos. En particular, la probabilidad de cualquier evento. \\(\\{S_1,S_2,...S_n\\}\\) es la suma de las probabilidades de sus elementos: \\[P(\\{S_1,S_2,...S_n\\}) = P(S_1)+P(S_2)+...+P(S_n)\\] 2.6.4 Ley de Probabilidad Discreta Uniforme Si el espacio muestral consiste de \\(n\\) posibles resultados con igual probabilidad de ocurrencia, entonces la probabilidad de cualquier evneto \\(A\\), está dada por: \\[P(A)= \\frac{\\text{Número de elementos de A}}{n}\\] 2.6.5 Función Densidad de Probabilidad También conocida como Probability Density Function (pdf) o Probability Mass Function (pmf). Para una variable aleatoria o evento discreto, se define como: \\[f(x) = P(X=x)\\] Muchos procesos aleatorios vienen descritos por variables de forma que son conocidas las probabilidades en itnervalos. La integral definida de la función de densidad en dichos intervalos coincide con la probabilidad de los mismos. Es decir, identificamos la probabilida de un intervalo con el área bajo la función de densidad. 2.6.6 Acumulativa pdf (Comulative pdf) Se define como la suma de los resultados parcials de los eventos que validan el experimento: \\[cpdf = \\sum_{i}(P(x_i=x))\\] Para los \\(x_i\\) que validan el experimento. 2.6.7 Función de Distribución Es la función que asocia a cada valor de una variable, la probabilidad acumulada de los valores inferiores o iguales. Piénsalo como la generalización de las frecuencias acumuladas. Diagrama integral. A los valores extremadamente bajos les corresponden valores de la función de distribución cercanos a cero. A los valores extremadamente altos les corresponden valores de la función de distribución cercanos a uno. Se encuéntra en los artículos y aplicaciones en forma de valor p, significación. 2.6.7.1 Para qué sirve la función de distribución Contrastar lo anómalo de una observación concreta. Sé que una persona de altura 210 cm es “anómala” porque la función de distribución en 210 es muy alta. Sé que una persona adulta que mida menos de 140 cm es “anómala” porque la función de distribución es muy baja para 140cm. Sé que una persona que mida 170 cm no posee una altura nada extraña pues su función de distribución es aproximadamente 0.5 Ésto se puede relacional con la idea de cuantil. En otro contexto (contrastes de hipótesis) podremos observar unos resultados experimentales y contrastar lo “anómalos” que son en conjunto con respecto a una hipótesis de terminada. 2.7 Probabilidad Condicional La siguiente ecuación: \\[P(A/B) = \\frac{P(A\\cap B)}{P(B)}\\] Puede también ser expresada en los términos de probabilidad condicional secuencial: \\[P(A\\cap B) = P(A/B)*P(B)\\] Siempre y cuando \\(P(B)&gt;0\\) Si los posibles resultados tienen la misma probabilidad de ocurrencia entonces: \\[P(A/B) = \\frac{\\text{No. de elementos en } A\\cap B}{\\text{No. de elementos en }B}\\] Importante: El conjunto universo en la probabilidad condicional (dado un evento anterior) cambia de \\(\\Omega\\) al subconjunto \\(B\\). 2.7.0.1 Ejemplos: Considerar el lanzamiento de 2 dados: \\[A = \\{\\text{la suma es}\\geq 8 \\space y \\space \\leq 10\\}\\] \\[B = \\{\\text{el dado 1}=6 \\} \\therefore P(B) = \\frac {1}{6}\\] Recordando los conceptos de combinaciones: \\(C = n^m\\) cuando \\(n\\) es el número de posibles resultados y \\(m\\), el número de experimentos. para éste ejemplo en particular las combinaciones posibles son \\(6^2 = 36\\), entonces: \\[P(A\\cap B) = \\frac {3}{36} \\therefore P(A/B) = \\frac {\\frac {3}{36}}{\\frac {1}{6}} = \\frac {18}{36} = \\frac {1}{2}\\] Es importante que la probabilidad condicional es mayor a la probabilidad independiente de previos eventos, porque el universo en la condicional disminuye de \\(\\Omega\\) a un subconjunto, en éste caso \\(B\\). La probabilidad de que, al lanzar los dados del ejemplo anterior, la suma sea \\(\\geq 8\\) y \\(\\leq 10\\) ó \\(A = \\{\\geq 8 \\space y \\space \\leq 10\\}.\\) A ésta probabilidad la denotaremos como \\(P(A)\\), al realizar el experimento manual obtenemos la siguiente imagen: Se puede observar que se han indicado las combinaciones que cumplen con una suma \\(\\geq 8\\) y \\(\\leq 10\\) ó \\(A = \\{\\geq 8 \\space y \\space \\leq 10\\}\\). Por lo tanto: \\[P(A) = \\frac {12}{36} = \\frac {1}{3}\\]. Otra manera de entender éste concepto es visualizarlo a través de una matriz: Con la visualización, es sencillo llegar a la conclusión \\(A = \\frac {12}{36} = \\frac {1}{3}\\) En el lanzamiento sucesivo de 3 monedas, queremos saber la probabilidad condicional, \\(P(A/B)\\), cuando \\(A\\) y \\(B\\) son los eventos: \\[A = \\{\\text{aparecen más soles que águilas}\\}\\] \\[B = \\{\\text{el primer lanzamiento cae sol}\\}\\] La visualización de todas las posibilidades de éste fenómeno se puede apreciar en la siguiente figura: Derivado del árbol en la imagen se pueden determinar las probabilidades de los siguientes subconjuntos: \\[P(A) = \\{\\text{sss, ssa, sas, ass }\\} = \\frac {4}{8} = \\frac {1}{2}\\] \\[P(B) = \\{\\text{sss, ssa, sas, saa}\\} = \\frac{4}{8} = \\frac{1}{2}\\] \\[P(A\\cap B) = \\{\\text{sss, ssa, sas}\\} = \\frac{3}{8}\\] \\[\\therefore\\] \\[P(A/B) = \\frac {P(A\\cap B)}{P(B)} = \\frac {\\frac {3}{8}}{\\frac {1}{2}} = \\frac {6}{8} = \\frac {3}{4}\\] Nótese el la mayor probabilidad de \\(P(A/B)\\) dado que el universo pasó de ser \\(\\Omega\\) al subconjunto \\(B\\). 2.7.1 Repaso de combinaciones y permutaciones La cantidad total de combinaciones posibles en un experimento se puede calcular como \\(n^m\\) donde: \\[n = \\text{posibles resutlados}\\] \\[m = \\text{número de experimentos a realizar}\\] Cuando de un número de combinaciones se elige una muestra \\(r\\) y el orden en el que éstas se elige no importa (combinaciones): \\[P = \\left( \\begin{array}{c} n \\\\ r \\end{array} \\right) = \\frac {n!}{r!(n-r)!}\\] Cuando de un numero de combinaciones se elige una muestra \\(r\\) y el orden en el que ésta se elige SI importa (permutaciones): \\[P = \\left( \\begin{array}{c} n \\\\ r \\end{array} \\right) = \\frac {n!}{(n-r)!}\\] 2.7.1.1 Ejemplo: Al lanzar 8 dados, ¿cuántas combinaciones diferentes son posibles? \\[C = 6^8\\] PENDIENTE REVISAR- Si elijo el resultado de 2 dados de los 8 lanzados, ¿de cuántas maneras distintas los puedo elegir?: \\[P = \\left( \\begin{array}{c} 8 \\\\ 2 \\end{array} \\right) = \\frac {n!}{r!(n-r)!}\\] Siempre y cuando \\(A_1\\) y \\(A_2\\) sean eventos disjuntos (\\(P(A_1\\cap A_2) = \\emptyset\\)) podemos entonces decir que: \\[P = (A_1 \\cap A_2 \\vert B) = \\frac {P(A_1 \\cap B)}{P(B)}+ \\frac {P(A_2 \\cap B)}{P(B)} = P(A_1/B) + P(A_2/B)\\] 2.7.1.2 Ejemplo Tomando datos del experimento anterior con monedas: \\[A_1 = \\{\\text{aparecen más soles que águilas}\\} = \\{\\text{sss, ssa, sas, ass}\\}\\] \\[A_2 = \\{\\text{aparecen más águilas que soles}\\} = \\{\\text{ssa, asa, aas, aaa}\\}\\] \\[B = \\{\\text{el primer lanzamiento es sol}\\} = \\{\\text{sss, ssa, sas, saa}\\}\\] \\[A_1 \\cap A_2 = \\{\\emptyset\\}\\] \\[\\therefore\\] \\[P(A_1) = \\frac {4}{8} = \\frac {1}{4}\\space \\&amp; \\space P(A_2) = \\frac {4}{8} = \\frac {1}{4}\\] \\[P(A_1\\cap B) = \\frac{3}{8}\\] \\[P(A_2\\cap B) = \\frac{1}{8}\\] \\[P(A_1\\cup A_2\\vert B) = \\frac {\\frac{3}{8}}{\\frac{1}{2}} + \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{6}{8} + \\frac{2}{8} = 1\\] \\[P(A_1\\cup A_2\\vert B) = 1 \\space \\therefore\\] \\[P(A_1\\cup A_2...\\cup A_i \\vert B) = P(A_1/B) + P(A_2/B)+...+ P(A_i/B)\\] Siempre y cuando \\(\\{A_1\\cap A_2...\\cap A_i\\}= \\emptyset\\). 2.7.2 Independencia Si: \\[P(A\\cap B) = P(A)*P(B)\\] Se dice que \\(A\\) y \\(B\\) son eventos independientes. Distinto a ser eventos disjuntos: \\(P(A\\cap B) = \\emptyset\\). Entonces, asumiendo independencia obetnemos: \\[P(A/B) = \\frac {P(A\\cap B)}{P(B)} = \\frac{P(A)P(B)}{P(B)} = P(A)\\] La ecuación anterior demuestra que, los eventos independientes no afectan la probabilidad uno del otro. 2.8 Modelos de Distribución Probabilística y variables aleatorias. Hay variables aleatorias que aparecen con frecuencia: Experimentos dicotómicos. Bernoulli Contar éxitos en experimentos dicotómicos repetidos: Binomial Poisson (sucesos raros) Y en otras muchas ocasiones: Distribución normal (gaussiana, campana, …) 2.8.1 Distribución de Bernoulli (López 2013) Tenemos un experimento de Bernoulli si al realizar un experimento sólo son posibles dos resultados: \\(x = 1\\) (éxito, con probabilidad p) \\(x = 0\\) (fracaso, con probabilidad q = 1 - p) Ejemplos: Lanzar una moneda y que salga cara: \\(p = \\frac{1}{2}\\). Elegir una persona de la población y que esté enfermo: $p = = $ prevalencia de la enferemdad. Aplicar un tratamiento a un enfermo y que éste se cure: $p = 95% = $ probabilidad de que el individuo se cure. Como se aprecia, en experimentos donde el resultado es dicotómico, la variable queda perfectamente determinada conociendo el parámetro p. 2.8.2 Distribución binomial (López 2013) Función de probabilidad: \\[P[X = k]= \\left( \\begin{array}{c} x \\\\ y \\end{array}\\right)p^kq^{n-k}, 0 \\leq k \\leq n\\] En ésta distribución hay problemas de cálculo si \\(n\\) es grande y/o \\(p\\) cercano a \\(0\\) ó \\(1\\). Media: \\(\\mu = np\\) Varianza: \\(\\sigma^2 = npq\\) AQUI PONER IMAGEN 2.8.3 Distribución Binomial (López 2013) Si se repite un número fijo de veces, \\(n\\), un experimento de Bernoulli con parámetro \\(p\\), el número de éxitos sigue una distribución binomial de parámetros (\\(n,p\\)). Ejemplos: Lanzar una moneda 10 veces y contar las caras: Bin(\\(n = 10\\), \\(p = \\frac{1}{2}\\)) Lanzar una moneda 100 veces y contar las caras: Bin(\\(n = 100\\), \\(p = \\frac{1}{2}\\)). En éste caso es mejor utilizar una distribución normal. El número de personas que enfermará (en una población de 500, 000 personas) de una enfermedad que desarrolla una cada de 2000 personas: Bin(\\(n = 500,000\\), \\(p = \\frac{1}{2000}\\)) Es difícil hacer cálculos con las cantidades del ejemplo anterior. El modelo de Poisson será mas adecuado en éstas ocaciones. 2.8.4 Distribución normal de Gauss (López 2013) Aparece de manera natural: Errores de medida. Distancia de frenado. Altura, peso, propensión al crimen … Distribuciones binomiales con \\(n\\) grande (n &gt; 30) y ’p con (np&gt;5) ó (nq &gt; 5). Está caracterizada por dos parámetros: La media (\\(\\mu\\)) y La desviación estándar/ típica (\\(\\sigma\\)) Función de densidad de la distribución normal: \\[f(x) = \\frac {1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})}\\] References "],
["recoleccion-de-datos.html", "Capítulo 3 Recolección de Datos 3.1 Definiciones 3.2 Tipos de Muestreo 3.3 Tipos de Estudios", " Capítulo 3 Recolección de Datos 3.1 Definiciones A primary goal of statistical studies is to collect data that can then be used to make informed decisions. It should come as no surprise that the ability to make good decisions depends on the quality of the information available. Datos: colección de información sobre las variables de interés. Para poder tener una colección habrá que definir la población de donde saldrán éstos datos: Población: Conjunto de datos, objetos, humanos, experiencias, étc. Población infinita: Aquella que no se puede o es difícil de contar. (población difícil o no contable) Población finita: se puede contar y/o medir. Población muestreada: subconjunto de la población que reúne todas las características (parámetros) de la población. Parámetro: unidad de medida de los atributos, características de la población. Muestra: Subconjunto de la población. Estadísticos: unidad de medida de un atributo o característica de la muestra. Censo: cuando la población se analiza en su totalidad. A study is an observational study if the investigator observes characteristics of a sample selected from one or more existing populations. The goal of an observational study is usually to draw conclusions about the corresponding population or about differences between two or more populations. In a well-designed observational study, the sample is selected in a way that is designed to produce a sample that is representative of the population. A study is an experiment if the investigator observes how a response variable behaves when one or more explanatory variables, also called factors, are manipulated. The usual goal of an experiment is to determine the effect of the manipulated explanatory variables (factors) on the response variable. In a well-designed experiment, the composition of the groups that will be exposed to different experimental conditions is determined by random assignment. 3.2 Tipos de Muestreo Advantages of sampling methods: Reduced cost. Greater speed. Greater Scope Greater Accuracy 3.2.0.1 Types of sampling 3.2.0.1.1 Non Probabilistic Techniques Muestreo por Conveniencia: Los encuestados son seleccionados porque estaban en el lugar preciso en el momento adecuado. Muestreo por Juicio: Los encuestados son seleccionados siguiendo el criterio del investigador, basándose en su conocimiento de la población objetivo. Muestreo Encadenado: Se selecciona a una muestra que sirve como punto de partida para otra muestra. Se utiliza cuando, por la naturaleza delicada de la pregunta o la dificultad de encontrar a los encuestados, es necesario que el encuestado nos dirija a otro. Un ejemplo típico de la utilidad de este método sería la investigación sobre hábitos de conducta moralmente no aceptados por la sociedad. Muestreo por Cuotas: En este caso el investigador tiene una información más detallada de la distribución de la población según algunas variables que están relacionadas con la variable a estudiar. De acuerdo con estas variables se divide la población en estratos y se entrevista un número determinado en cada estrato. 3.2.0.1.2 Probabilistic Techniques Muestreo Aleatorio Simple (MAS): Cada elemento de la población y cada posible muestra de un tamaño n tienen una probabilidad conocida e igual de ser seleccionados. Esta técnica requiere tener un listado exhaustivo de todos los elementos de la población objetivo. Errores que arruinan el muestreo aleatorio simple (buscar evitarlos siempre): Tendencias. Prejuicios. Parcialidades. Muestreo Sistemático: Se elige un primer elemento aleatoriamente y a continuación todos los siguientes cada n posiciones. Así, si por ejemplo queremos seleccionar 20 individuos de entre 100, el primer paso es seleccionar un número aleatorio entre 1 y 5 (ya que 100/20=5), digamos el 3; a continuación seleccionamos los individuos 3, 8, 13, 18,… 93, 98. Esto implica la ordenación de todos los elementos de la población, si bien el criterio de ordenación no debe guardar ninguna relación con el fenómeno sociológico a estudiar. Muestreo Estratificado: Requiere al menos dos etapas. En una primera etapa la población objetivo se divide en estratos según las variables que se consideran relacionadas con el fenómeno sociológico a estudiar. La segunda fase consiste en seleccionar aleatoriamente una muestra dentro de cada estrato. Muestreo por conglomerados (clusters): Monoetápico: La población es dividida en conglomerados (barrios, manzanas), seleccionando un grupo de ellos con probabilidad proporcional a su importancia. Una vez se tienen estos conglomerados se encuesta a todos los elementos del conglomerado (todos los vecinos de las manzanas B, H y G). Bietápico: Igual que el anterior pero en lugar de encuestar a todos los elementos del conglomerado se selecciona una muestra. 1-in-\\(k\\) systematic sampling/ muestreo sistemático 1-en-\\(k\\): Consiste en seleccionar cada \\(k\\)-ésima unidad. Útil para muestreo de artículos en líneas de producción. Ejemplos: Muestreo con reemplazo: En un lote de 1000 artículos. Agarro 20 al hazar, si \\(\\leq 4\\) están defectuosos, acepto el lote. Muestreo sin remplazo: Después de sacar 20, vuelvo a muestrear a los 980 restantes (con ésta técnica la muestra aumenta, dado que \\(\\Omega\\) disminuye y la cantidad de artículos defectuosos siguen siendo los mismos en cantidad) \\(\\therefore\\) Tomo 30 artículos, si \\(\\leq 4\\) resultan defectuosos entonces rechazo el lote. 3.2.1 Resumen de Muestreos 3.3 Tipos de Estudios 3.3.0.1 Correlation Analysis (CA) El análisis de correlaciones es muy útil para el investigador para determinar si existe alguna relación o asociación entre diversas variables de interés antes de continuar con un análisis más sofisticado de causa-efecto. El análisis de correlaciones también constituye un insumo fundamental para realizar diversos análisis estadísticos más avanzados como el análisis factorial y el análisis de confiabilidad. Estos análisis son utilizados por los investigadores para determinar la validez y confiabilidad de las encuestas de actitud. 3.3.0.1.1 Statistical assumption of CA El análisis más común es el análisis de correlación de Pearson (Pearson product moment correlacion coefficient). Este tipo de análisis presupone que las variables son ordinales o continuas y que la distribución de estas variables se acerca a la distribución normal (Bell shape curveo “curva de campana”). Es aconsejable que antes de proceder al análisis de correlación de las variables, el investigador estime las estadísticas descriptivas correspondientes para determinar si se cumplen estos supuestos. Estudio observacional: Estudio experimental: Recordar el concepto de efecto placebo, para evitar éste efecto se utilizan grupos control en los estudios. "],
["data-preparation.html", "Capítulo 4 Data Preparation 4.1 Data Transformation", " Capítulo 4 Data Preparation It is important to remember that one of the first steps in data preparation is exploring the distribution of the values in each considered variable. Generally, a normal distribution is prefered over other distributions because there are more tools available to analize data that behaves in a normal way. 4.1 Data Transformation Transactional processes and most metrics that involve time measurements exist with non-normal distributions. Some examples: Mean time to repair HVAC equipment. Admissions cycle time for college applicants. Days sales outstanding. Waiting times at a bank or physicians office. Time being treated in a hospital emergency room. Usted puede transformar los datos usando muchas funciones, tales como raíz cuadrada, logaritmo, potencia, recíproca o arcoseno. La transformación de Box-Cox es fácil de comprender, pero es muy limitada y, con frecuencia, no determina una transformación adecuada. Además, está disponible solo para datos que son positivos. La transformación de Johnson. La función de transformación de Johnson es más complicada, pero es muy efectiva para determinar una transformación adecuada. "],
["exploracion-de-datos-y-visualizacion.html", "Capítulo 5 Exploración de Datos y Visualización 5.1 General Concetps 5.2 Medidas de Tendencia Central 5.3 Medidas de Variabilidad o Dispersión 5.4 Desviación Estándar o Típica 5.5 Propiedades de la Media y Varianza 5.6 Curtosis 5.7 Índice de Asimetría 5.8 Quantile-Quantile Plot (Q-Q plot) 5.9 Boxplots 5.10 Using different graphics in the same exploration 5.11 Other kinds of distributions", " Capítulo 5 Exploración de Datos y Visualización 5.1 General Concetps An “outlier” is an extremely high or an extremely low data value when compared with the rest of the data values. Scott’s rule for the bin width w = 3.5σ/ 3√n, where σ is the standard deviation for the entire data set and n is the number of points. This rule assumes that the data follows a Gaussian distribution; otherwise, it is likely to give a bin width that is too wide. Histograms can be either normalized or unnormalized. In an unnormalized histogram, the value plotted for each bin is the absolute count of events in that bin. In a normalized histogram, we divide each count by the total number of points in the data set, so that the value for each bin becomes the fraction of points in that bin. If we want the percentage of points per bin instead, we simply multiply the fraction by 100. 5.1.1 Key points to explore in cuantitative data to begin with: Shape (graphs done with data) Symmetric? Left or right skewed? Unimodal? Bi-modal? Center Mean Median Mode Spread Range (max-min values) IQR (interquartile range): \\(Q_3 - Q_1\\) Outliers 5.2 Medidas de Tendencia Central Criterios a seguir: Calcular la media (entre otras razones porque es el mejor estimador del parámetro poblacional). Si no puede calcularse (p.e. variables ordinales, valores extremos) obtener Mediana. Si no puede obtenerse Mediana (p.e. datos nominales, intervalos abiertos con más del 50% de sujetos) obtener Moda. En algunos casos los tres indicadores pueden dar valores similares pero no necesariamente ha de ser así. Mdn = X = Mo, esto solo sucedera sii la distribución es simétrica: 5.3 Medidas de Variabilidad o Dispersión Asimetría positiva. Asimetría negativa. Para conseguir una visión completa y comprensiva de los datos obtenidos hay que complementar las medidas de tendencia central con otros estadísticos que reflejen otras propiedades. Por ejemplo, el grado en que los datos se parecen o diferencian entre sí, propiedad que se denomina variabilidad o variación. 5.3.1 Varianza Es el promedio de las distancias al cuadrado desde los valores en \\(X\\) hasta la media de \\(\\overline X\\) (es decir, de las puntuaciones diferenciales al cuadrado) en una muestra de \\(n\\) sujetos. Fórmulas: \\[S^2_x = \\frac{\\sum(X_i - \\overline X)^2}{N}\\] \\[S^2_x = \\frac{\\sum x^2_i}{N}\\] Fórmula alternativa: \\[S^2_x = \\frac{\\sum X^2_i}{N}-\\overline X^2\\] 5.4 Desviación Estándar o Típica 5.5 Propiedades de la Media y Varianza La media puede tomarse como cualquier valor mientras que \\(S^2X\\) y \\(SX\\) son siempre positivas, siendo su valor mínimo \\(0\\). Si tenemos una misma variable \\(X\\) que ha sido medida en \\(k\\) grupos y conocemos las medidas y varianzas en cada grupo, entonces podemos calcular los estadísticos globalesÑ 5.6 Curtosis Sirve para analizar el grado de concentración que presentan los valores de una variable analizada alrededor de la zona centrla de la distribución de frecuencias, sin necesidad de generar un gráfico. 5.7 Índice de Asimetría La asimetría de una distribución hace referencia al grado en que los datos se reparten por encima y por debajo de la tendencia central. 5.8 Quantile-Quantile Plot (Q-Q plot) It is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a normal or exponential. For example, if we run a statistical analysis that assumes our dependent variable is normally distributed, we can use a Normal Q-Q plot to check that assumption. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. 5.8.1 Q-Q plot of a uniform distribution 5.8.2 Q-Q plot of a Poisson distribution 5.8.3 Understanding type of distribution through Q-Q plots 5.9 Boxplots Officially known as box-and-whisker plots, also called boxplots, for short, this is a technique for exploratory data analysis devised by Tukey (1977). One can see the spread and symmetry of a distribution at a glance, and the position of any extreme scores. Hinges: These are the sides of the box in a boxplot, corresponding approximately to the 25th and 75th percentiles of the distribution. H-spread: The distance between the two hinges (i.e., the width of the box) in a boxplot. Inner fences: Locations on either side of the box (in a boxplot) that are 1.5 times the H-spread from each hinge. The distance between the upper and lower inner fence is four times the H-spread. Adjacent values: The upper adjacent value is the highest score in the distribution that is not higher than the upper inner fence, and the lower adjacent value is similarly defined in terms of the lower inner fence of a boxplot. The upper whisker is drawn from the upper hinge to the upper adjacent value, and the lower whisker is drawn from the lower hinge to the lower adjacent value. Outlier: Defined in general as an extreme score standing by itself in a distribution, an outlier is more specifically defined in the context of a boxplot. In terms of a boxplot, an outlier is any score that is beyond the reach of the whiskers on either side. The outliers are indicated as points in the boxplot. Two or more box plots drawn on the same Y-axis. These are often useful in comparing features of distributions. An example portraying the times it took samples of women and men to do a task is shown below. 5.9.1 More elaborate Boxplots Means indicated in green lines rahter than plus signs. Mean of all scores indicated by a gray line. individual scores are represented by dots. Since the scores have been rounded to the nearest second, any given dot might represent more than one score. One box is wider than the other one because the widths of the boxes are proportional to the number of subjects. If all the dots want to be seen we need to add some jitter to the boxplot: 5.10 Using different graphics in the same exploration 5.11 Other kinds of distributions "],
["analisis-de-los-datos.html", "Capítulo 6 Análisis de los Datos 6.1 Análisis Univariante 6.2 Análisis Vibariante 6.3 Análisis Multivariante 6.4 Pruebas de Hipótesis", " Capítulo 6 Análisis de los Datos 6.1 Análisis Univariante Estudia la distribución individual de cada variable. Este análisis se centra en dos aspectos: La tendencia central de la distribución y su dispersión. En el primer caso hablamos de un valor característico o medio de la distribución, en el segundo de la variabilidad interna de los datos. Según el tipo de variables proceden los siguientes análisis: Variables nominales. Para este tipo de variables el análisis se limita a las frecuencias de cada categoría. Se suele expresar en porcentajes. Variables ordinales. La tendencia central se mide con los estadísticos mediana y moda (pero no la media, ya que ésta implica distancias comparables), mientras que para la dispersión podemos utilizar un diagrama de frecuencias (histograma). Variables métricas. Para el análisis de la tendencia central se utiliza por lo general la media, si bien es aconsejable utilizar la mediana cuando nos encontramos con unos pocos valores extremos cuya magnitud difiere ampliamente del resto (son mucho mayores o mucho menores que la mayoría). Para estudiar el grado de dispersión recurrimos a la desviación típica o la varianza. Es posible estudiar además del momento de primer orden de la distribución (tendencia central) y el momento de segundo orden (dispersión), el momento de tercer orden (simetría) y el de cuarto orden (achatamiento). No es frecuente estudiar momentos de orden superior. 6.2 Análisis Vibariante Distribución normal de las variables. Para llevar a cabo pruebas de estadística paramétrica se asume que la variable estudiada de la población sigue una dsitribución normal. Uniformidad de la varianza. Se requiere que la varianza de una variable no dependa del nivel de otra variable. Escala de medida. Las variables deben medirse en una escala métrica. Independencia. Las respuestas de un sujeto no dependen de las de otro. 6.3 Análisis Multivariante Análisis de la varianza (ANOVA II). Se analiza el efecto de dos variables nominales (llamadas factores) sobre una variable métrica. Ejemplo: ¿Depende el rendimiento de una máquina (variable métrica) del modelo de la máquina (factor 1) y del operario que la manjea (factor 2)? Análisis multivariante de la varianza (MANOVA). A diferencia de ANOVA, éste análisis considera simultáneamente dos o más variables dependeintes. En este caso, ¿por qué no utilizar ANOVA para cada una de las variables dependeintes? La respuesta es simple: porque es posible que exista una diferencia entre grupos que sólo se ponga en evidencia considerando varias variables dependientes simultáneamente. Análisis discriminante. La idea central de ésta técnica consiste en ponderar las diferentes variables a la hora de asignar un elemento a un grupo u otro. Por tanto, la variable dependiente es nominal mientras que las explicativas pueden ser de cualquier tipo. Ejemplo: ¿Podríamos predecir el tipo de coche (variable nominal) que un consumidor compraría en función de su nivel de ingresos (variable ordinal), sexo (variable nominal) y edad (variable métrica)? Análisis de regresión lineal múltiple. Se estudia el efecto simultáneo de un conjunto de variables de cualquier tipo (variables explicativas) sobre una variable métrica (variable dependiente). Ejemplo: ¿Cómo influye en el rendimiento de una máquina el modelo de la misma, el operario que la maneja, su antigüedad y el turno de trabajo? 6.4 Pruebas de Hipótesis 6.4.1 Generalidades 6.4.2 Errores Tipo I y Tipo II 6.4.3 Prueba de una cola Queremos determinar si el tratamiento A contra una plaga reduce significativamente la incidencia de la misma sobre el cultivo. Para ello medimos el número de gusanos por planta en 5 parcelas con tratamiento y en 5 parcelas sin tratamiento. En este caso, lo lógico es pensar que el tratamiento no va a incrementar el número de gusanos y que, si tiene algún efecto, será reducir dicho número. Este es un caso de prueba de una cola. &lt; br&gt; La hipótesis se formula como sigue: \\[H_0: \\mu_0 = \\mu_A \\text{ &lt;--- (no existe diferencia entre las dos medias)}\\] \\[H_A: \\mu_0 &gt; \\mu_A \\text{ &lt;--- (la media de las fincas sin tratar es mayor que la media de las fincas tratadas)}\\] 6.4.4 Dos colas Tenemos una prueba de dos colas cuando la prueba de hipótesis queda expresada como el siguiente ejemplo: \\[H_0: \\mu_A = \\mu_B \\text{ &lt;--- (no existen diferencias entre las dos medias)}\\] \\[H_0: \\mu_A \\neq \\mu_B \\text{ &lt;--- (las medias de las fincas tratadas con A o B son distintas)}\\] ¿Qué implica el hecho de considerar una prueba con una o dos colas? &lt; br&gt; En el primer caso, una cola, el valor crítico que determina rechazar o no la hipótesis nula deja a la derecha la totalidad del nivel de significancia (\\(\\alpha\\)). En el caso de una prueba de dos colas, este vlaor deja a la derecha (o a la izquierda para un valor negativo) la mitad de esta probabilidad (\\(\\alpha/2\\)). Esto implica que en una prueba de dos colas es más difícil rechazar la hipótesis nula ya que el valor crítico es mayor. Por ejemplo, en una distribución normal el valor crítico que deja el 5% de la probabilidad a la derecha es igual a 1.65, sin embargo este valor aumenta hasta 1.96 con sólo el 2.5% de la superficie a la derecha. A continuación una imagen que ejemplifica éste concepto: "],
["bayesian-statistics.html", "Capítulo 7 Bayesian Statistics 7.1 Math for understanding the Bayesian approach 7.2 Conceptos sueltos que debo cubrir 7.3 Entropy and Information Gain 7.4 Bayesian Linear Regression 7.5 Examples of Bayesian models", " Capítulo 7 Bayesian Statistics 7.1 Math for understanding the Bayesian approach \\[P(A_k|B)=\\frac{P(A_k\\cap B)}{P(B)}\\] \\[P(B|A_k)=\\frac{P(B\\cap A_k)}{P(A_k)} \\therefore P(A_k|B)=\\frac{}{}\\] 7.1.1 Ejercicio Morgan asks Smith to pick a number between 20 and 50, Smith tells Morgan a characteristic of the number he picked (it is a multiple of 5). With this information, determine the probability of Smith’s pick being also a multiple of 3 using ven diagrams and conditional probability. Población total de 1000, una enfermedad se presenta en la población con prevalencia del 10%. Para poder 7.2 Conceptos sueltos que debo cubrir Tamaño Muestral Equivalente. Ajuste de Laplace. Laplace sugiere sumar un 1 (\\(\\frac{n_1+1}{n_1+n_0+2}\\)) \\[\\frac{n_1+\\alpha}{n_1+n_0+(\\alpha + \\beta)}\\] Redes bayesianas, escribir sobre ésto. \\[p(p|D) \\sim \\beta (\\alpha + n_1, \\beta +n_0)\\] \\[p(p) \\sim \\beta(\\alpha, \\beta)\\] Regresión Bayesiana. 7.3 Entropy and Information Gain 7.3.1 Key definitios Formula for Entropy \\[H(x) = -\\sum_{i=1}^{n}P(x_i)*log_2P(x_i)\\] Information Gain tells us how important a given attribute of the feature vectors is for predicting that characteristic. Mathematically it is expressed as follows: \\[Gain(x,y) = H(x)-\\sum_{v\\in Values(A_i)}P(A_i=v)H(S_v)\\] 7.3.2 Examples 7.3.2.1 Example 1 #Creating matrix test&lt;-matrix(c(346,74,124,26,470,100), nrow = 3, ncol = 2, byrow = TRUE) colnames(test) &lt;- c(&quot;count&quot;, &quot;%&quot;) rownames(test) &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Overall&quot;) test ## count % ## Yes 346 74 ## No 124 26 ## Overall 470 100 Following the formula for entropy shown in the previous section we can obtain the entropy value for this dataset: \\[H(dataset) = -[\\{P(yes)*log_2P(yes)+P(no)*log_2P(no)\\}] = [(0.74*log_2(0.74))+(0.26*log_2(0.26))]\\] This operation gives the following result: #Entropy for the node H &lt;- -((0.74*log(0.74, base = 2))+(0.26*log(0.26, base = 2))) H ## [1] 0.8267464 We can, therefore, conclude that the entropy in this node is 0.827 7.3.2.2 Example 2. Usig Entropy and Information Gain to classify We will use a very simple dataset: test2 &lt;- matrix(c(1,1,1,&quot;I&quot;,1,1,0,&quot;I&quot;,0,0,1,&quot;II&quot;,1,0,0,&quot;II&quot;), ncol = 4, nrow = 4, byrow = TRUE) colnames(test2) &lt;- c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;Class&quot;) test2 &lt;- as.data.frame(test2) test2 ## X Y Z Class ## 1 1 1 1 I ## 2 1 1 0 I ## 3 0 0 1 II ## 4 1 0 0 II The challenge is presented to use only one attribute to predict which Class each value belongs to. The approximation is done following these steps: Calculate entropy values for each attribute. Calculate information gain. Select the attribute with higher information gain result. X1 &lt;- table(test2$X == 1) X0 &lt;- table(test2$X == 0) data.frame(X1, X0) ## Var1 Freq Var1.1 Freq.1 ## 1 FALSE 1 FALSE 3 ## 2 TRUE 3 TRUE 1 Step 1 Entropy for attribute X \\[H(X_1) = -\\left[ \\frac{2}{3}*log_2\\frac{2}{3} + \\frac{1}{3}*log_2\\frac{1}{3} \\right] = 0.9183 \\] #X1 &lt;- -((2/3*log(2/3, base = 2))+(1/3*log(1/3, base = 2))) #X0 &lt;- -((1/1*log(1/1, base = 2))) #Hx &lt;- Information Gain for attribute X \\[I = 1-P(Y=1)*H(Y=1)-P(X=0)*H(X=0)=1-\\left( \\frac{3}{4}*0.9183\\right)-\\left(\\frac{1}{4}*0\\right)=1-0.688725-0\\] \\[= \\boldsymbol{0.311275}\\] Step 2 Entropy for attribute Y \\[H(X_1) = -\\left[ \\frac{2}{2}*log_2\\frac{2}{2} \\right] = 0 \\] Information Gain for attribute Y \\[I = 1-P(Y=1)*H(Y=1)-P(Y=0)*H(Y=0)=1- 0-0= \\boldsymbol{1}\\] Step 3 Entropy for attribute Z \\[H(Z_1) = -\\left[ \\frac{1}{2}*log_2\\frac{1}{2}\\right] = 0.5 \\] Information Gain for attribute Z \\[I = 1-P(Z=1)*H(Z=1)-P(Z=0)*H(Z=0)=1- 0.5-0.5= \\boldsymbol{0}\\] According to the results these are the corresponding information gain results for each attribute: X: 0.311275 Y: 1 Z: 0 We can then safely state that using attribute Y to predict the target variable Class is the best option based on information gain. 7.4 Bayesian Linear Regression Before exploring the Bayesian approach to regressions, it is important to refresh our memory and review the traditional approach to regressions. 7.4.1 Linear Regression For this we will use the example of the cereals data set (Data and Library, n.d.). library(readr) cereals &lt;- read_delim(&quot;D:/Dropbox/MsC UABC/2o Semestre/Clases/Estadistica/estadistica-syllabus/datasets/cereals.csv&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) str(cereals) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 77 obs. of 16 variables: ## $ name : chr &quot;100%_Bran&quot; &quot;100%_Natural_Bran&quot; &quot;All-Bran&quot; &quot;All-Bran_with_Extra_Fiber&quot; ... ## $ mfr : chr &quot;N&quot; &quot;Q&quot; &quot;K&quot; &quot;K&quot; ... ## $ type : chr &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; ... ## $ calories: int 70 120 70 50 110 110 110 130 90 90 ... ## $ protein : int 4 3 4 4 2 2 2 3 2 3 ... ## $ fat : int 1 5 1 0 2 2 0 2 1 0 ... ## $ sodium : int 130 15 260 140 200 180 125 210 200 210 ... ## $ fiber : num 10 2 9 14 1 1.5 1 2 4 5 ... ## $ carbo : num 5 8 7 8 14 10.5 11 18 15 13 ... ## $ sugars : int 6 8 5 0 8 10 14 8 6 5 ... ## $ potass : int 280 135 320 330 -1 70 30 100 125 190 ... ## $ vitamins: int 25 0 25 25 25 25 25 25 25 25 ... ## $ shelf : int 3 3 3 3 3 1 2 3 1 3 ... ## $ weight : num 1 1 1 1 1 1 1 1.33 1 1 ... ## $ cups : num 0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ... ## $ rating : num 68.4 34 59.4 93.7 34.4 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 16 ## .. ..$ name : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ mfr : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ type : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ calories: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ protein : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ fat : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ sodium : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ fiber : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ carbo : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ sugars : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ potass : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ vitamins: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ shelf : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ weight : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ cups : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ rating : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; 7.4.1.1 Data Preparation As we can see, this dataset needs som data preparation before a linear regression model can be generated. #Converting to numeric data those variables that need to be read as such&gt; cereals$calories &lt;- as.numeric(cereals$calories) cereals$protein &lt;- as.numeric(cereals$protein) cereals$fat &lt;- as.numeric(cereals$fat) cereals$sodium &lt;- as.numeric(cereals$sodium) cereals$fiber &lt;- as.numeric(cereals$fiber) cereals$carbo &lt;- as.numeric(cereals$carbo) cereals$sugars &lt;- as.numeric(cereals$sugars) cereals$potass &lt;- as.numeric(cereals$potass) cereals$vitamins &lt;- as.character(cereals$vitamins) cereals$weight &lt;- as.numeric(cereals$weight) cereals$cups &lt;- as.numeric(cereals$cups) cereals$rating &lt;- as.numeric(cereals$rating) str(cereals) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 77 obs. of 16 variables: ## $ name : chr &quot;100%_Bran&quot; &quot;100%_Natural_Bran&quot; &quot;All-Bran&quot; &quot;All-Bran_with_Extra_Fiber&quot; ... ## $ mfr : chr &quot;N&quot; &quot;Q&quot; &quot;K&quot; &quot;K&quot; ... ## $ type : chr &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; ... ## $ calories: num 70 120 70 50 110 110 110 130 90 90 ... ## $ protein : num 4 3 4 4 2 2 2 3 2 3 ... ## $ fat : num 1 5 1 0 2 2 0 2 1 0 ... ## $ sodium : num 130 15 260 140 200 180 125 210 200 210 ... ## $ fiber : num 10 2 9 14 1 1.5 1 2 4 5 ... ## $ carbo : num 5 8 7 8 14 10.5 11 18 15 13 ... ## $ sugars : num 6 8 5 0 8 10 14 8 6 5 ... ## $ potass : num 280 135 320 330 -1 70 30 100 125 190 ... ## $ vitamins: chr &quot;25&quot; &quot;0&quot; &quot;25&quot; &quot;25&quot; ... ## $ shelf : int 3 3 3 3 3 1 2 3 1 3 ... ## $ weight : num 1 1 1 1 1 1 1 1.33 1 1 ... ## $ cups : num 0.33 1 0.33 0.5 0.75 0.75 1 0.75 0.67 0.67 ... ## $ rating : num 68.4 34 59.4 93.7 34.4 ... ## - attr(*, &quot;spec&quot;)=List of 2 ## ..$ cols :List of 16 ## .. ..$ name : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ mfr : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ type : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_character&quot; &quot;collector&quot; ## .. ..$ calories: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ protein : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ fat : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ sodium : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ fiber : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ carbo : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ sugars : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ potass : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ vitamins: list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ shelf : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_integer&quot; &quot;collector&quot; ## .. ..$ weight : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ cups : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## .. ..$ rating : list() ## .. .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_double&quot; &quot;collector&quot; ## ..$ default: list() ## .. ..- attr(*, &quot;class&quot;)= chr &quot;collector_guess&quot; &quot;collector&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;col_spec&quot; 7.4.1.2 Data Quality Report for Categorical Variables setwd(&quot;D:/Dropbox/MsC UABC/2o Semestre/Clases/Estadistica/estadistica-syllabus&quot;) source(&#39;QOfCategoricalF.R&#39;) DQRcat &lt;- QOfCategoricalF(cereals[,c(1,2,3,12,13)]) DQRcat ## Count Miss Card Mode ModeFrec ModePerc Mode2 ## name 77 0 77 100%_Bran 1 1.3% 100%_Natural_Bran ## mfr 77 0 7 K 23 29.87% G ## type 77 0 2 C 74 96.1% H ## vitamins 77 0 3 25 63 81.82% 0 ## shelf 77 0 3 3 36 46.75% 2 ## Mode2Frec Mode2Perc ## name 1 1.3% ## mfr 22 28.57% ## type 3 3.9% ## vitamins 8 10.39% ## shelf 21 27.27% 7.4.1.3 Data Quality Report for Continuous Variables source(&#39;QOfContinuousF.R&#39;) DQRcont &lt;- QOfContinuousF(cereals[,c(4:11, 14:16)]) DQRcont ## Count Miss Card Min Qrt1 Median Qrt3 Max Mean ## calories 77 0 11 50.00000 100.00 110.00 110.00 160.00000 106.88 ## protein 77 0 6 1.00000 2.00 3.00 3.00 6.00000 2.55 ## fat 77 0 5 0.00000 0.00 1.00 2.00 5.00000 1.01 ## sodium 77 0 27 0.00000 130.00 180.00 210.00 320.00000 159.68 ## fiber 77 0 13 0.00000 1.00 2.00 3.00 14.00000 2.15 ## carbo 77 0 22 -1.00000 12.00 14.00 17.00 23.00000 14.60 ## sugars 77 0 17 -1.00000 3.00 7.00 11.00 15.00000 6.92 ## potass 77 0 36 -1.00000 40.00 90.00 120.00 330.00000 96.08 ## weight 77 0 7 0.50000 1.00 1.00 1.00 1.50000 1.03 ## cups 77 0 12 0.25000 0.67 0.75 1.00 1.50000 0.82 ## rating 77 0 77 18.04285 33.17 40.40 50.83 93.70491 42.67 ## Sdev ## calories 19.48 ## protein 1.09 ## fat 1.01 ## sodium 83.83 ## fiber 2.38 ## carbo 4.28 ## sugars 4.44 ## potass 71.29 ## weight 0.15 ## cups 0.23 ## rating 14.05 7.4.2 Logistic Regression 7.4.2.1 Splitting train and test dataset #Spliting inot trainning and test datasets library(caTools) set.seed(123) split = sample.split(cereals$rating, SplitRatio = 2/3) training_set = subset(cereals, split == TRUE) test_set = subset(cereals, split == FALSE) # Feature Scaling #training_set[, c(4:11,14:16)] = scale(training_set[, c(4:11,14:16)]) # test_set[, 2:3] = scale(test_set[, 2:3]) 7.4.2.2 Fitting SLR to training set regressor &lt;- lm(formula = rating ~ sugars, data = cereals) summary(regressor) ## ## Call: ## lm(formula = rating ~ sugars, data = cereals) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.853 -5.677 -1.439 5.160 34.421 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 59.2844 1.9485 30.43 &lt; 2e-16 *** ## sugars -2.4008 0.2373 -10.12 1.15e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.196 on 75 degrees of freedom ## Multiple R-squared: 0.5771, Adjusted R-squared: 0.5715 ## F-statistic: 102.3 on 1 and 75 DF, p-value: 1.153e-15 7.4.2.3 Visualizing SLR on the copmlete cereals dataset library(ggplot2) ggplot() + geom_point(aes(x = cereals$sugars, y = cereals$rating), colour = &quot;red&quot;) + geom_line(aes(x = cereals$sugars, y = predict(regressor, newdata = cereals)), colour = &quot;blue&quot;) + ggtitle(&quot;Sugar vs Prediction of Rating variables&quot;) + theme(plot.title = element_text(hjust = 0.5)) + xlab(&quot;Amount of Sugars&quot;) + ylab(&quot;Raiting&quot;) 7.5 Examples of Bayesian models Taken from Rasmus Bååth´s blog. 7.5.1 Bayesian A testing for Swedish Fish Incorporated Swedish Fish Incorporated is the largest Swedish company delivering fish by mail order. They are now trying to get into the lucrative Danish market by selling one year Salmon subscriptions. The marketing department have done a pilot study and tried the following marketing method: A: Sending a mail with a colorful brochure that invites people to sign up for a one year salmon subscription. The marketing department sent out 16 mails of type A. Six Danes that received a mail signed up for one year of salmon and marketing now wants to know, how good is method A? 7.5.1.1 Question I: Build a Bayesian model that answers the question: What would the rate of sign-up be if method A was used on a larger number of people? Hint 1: The answer is not a single number but a distribution over probable rates of sign-up. Hint 2: As part of you generative model you’ll want to use the binomial distribution, which you can sample from in R using the rbinom(n, size, prob). The binomial distribution simulates the following process n times: The number of “successes” when performing size trials, where the probability of “success” is prob. Hint 3: A commonly used prior for the unknown probability of success in a binomial distribution is a uniform distribution from 0 to 1. You can draw from this distribution by running runif(1, min = 0, max = 1) Hint 4: Here is a code scaffold that you can build upon. First of all we need to declare the amount of draws our model will make and the prior distribution. # Number of random draws from the prior set.seed(123) n_draw &lt;- 100000 prior &lt;- runif(n_draw,0,1) # Here you sample n_draws draws from the prior hist(prior, col = &quot;pink&quot;, main = &quot;Hisogram of Prior Distribution for Strategy A&quot;) # It&#39;s always good to eyeball the prior to make sure it looks ok. Once this is finished we declare a generative model and simulate data with the parameters from the priors and the model. # Here you define the generative model generative_model &lt;- function(rate) { subscribers &lt;- rbinom(1, size = 16, prob = rate) subscribers } # Here you simulate data using the parameters from the prior and the # generative model subscribers &lt;- rep(NA, n_draw) for(i in 1:n_draw) { subscribers[i] &lt;- generative_model(prior[i]) } Next we work on filtering the draws that mached the data and generate a histogram of it, this will be the visualization of the Posterior Distribution. # Here you filter off all draws that do not match the data. posteriorA &lt;- prior[subscribers == 6] hist(posteriorA, col = &quot;coral2&quot;, xlim = c(0,1)) # Eyeball the posterior length(posteriorA) # See that we got enought draws left after the filtering. ## [1] 5897 # There are no rules here, but you probably want to aim # for &gt;1000 draws. # Now you can summarize the posterior, where a common summary is to take the mean # or the median posterior, and perhaps a 95% quantile interval. summary(posteriorA) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.06941 0.30652 0.38475 0.38865 0.46124 0.83222 As we can see from the previous calculations, if strategy A was used in a larger population we could expect a sign up rate from 20% to 60%, this result shows a considerable amount of uncertanty. 7.5.1.2 Question II: What’s the probability that method A is better than telemarketing? So marketing just told us that the rate of sign-up would be 20% if salmon subscribers were snared by a telemarketing campaign instead (to us it’s very unclear where marketing got this very precise number from). So given the model and the data that we developed in the last question, what’s the probability that method A has a higher rate of sign-up than telemarketing? Hint 1: If you have a vector of samples representing a probability distribution, which you should have from the last question, calculating the amount of probability above a certain value is done by simply counting the number of samples above that value and dividing by the total number of samples. Hint 2: The answer to this question is a one-liner. sum(posteriorA &gt; 0.2)/length(posteriorA) ## [1] 0.9659149 As we can see, it is more likely that strategy A will produce higher sign up rattings than telemarketing. 7.5.1.3 Question III: If method A was used on 100 people what would be the number of sign-ups? Hint 1: The answer is again not a single number but a distribution over probable numbers of sign-ups. Hint 2: As before, the binomial distribution is a good candidate for how many people that sign up out of the 100 possible. Hint 3: Make sure you don’t “throw away” uncertainty, for example by using a summary of the posterior distribution calculated in the first question. Use the full original posterior sample! Hint 4: The general patter when calculating “derivatives” of posterior samples is to go through the values one-by-one, and perform a transformation (say, plugging in the value in a binomial distribution), and collect the new values in a vector. # But since rbinom is vectorized we can simply write it like this: signupsA &lt;- rbinom(n = length(posteriorA), size = 100, prob = posteriorA) hist(signupsA, xlim = c(0, 100), col = &quot;pink&quot;, main = &quot;Histograms of Signups Projected in 100 Potential Clients With Strategy A&quot;) The probable number of signups if strategy A were to be used is between 20 and 60. 7.5.2 Calculating probabilies for a new marketing idea. The CEO of Swedish Fish Incorporated came with the “greatest idea” for marketing: along with the colorful brochure he wants to send a frozen salmon to the potential costumers. He has learned on or two things from marketing and had his idea already tested with 16 danes, out of these 10 sigend up for the service. What’s the probability of this strategy to generate signups in a larger population? First we need to generate the analysis for the new strategy: A prior distribution to start from, we’ll go with a uniform distribution: n_draw &lt;- 100000 prior &lt;- runif(n_draw,0,1) # Here you sample n_draws draws from the prior hist(prior, col = &quot;dodgerblue&quot;, main = &quot;Hisogram of Prior Distribution for Strategy B&quot;) # It&#39;s always good to eyeball the prior to make sure it looks ok. - The regenerative model and all the calculations will be done the same as strategy A, but instead of selecting those who resulted in 6, we will select those draws who resulted in 10. ## $breaks ## [1] 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 ## [15] 0.90 0.95 ## ## $counts ## [1] 1 25 67 143 299 483 727 920 1000 882 681 407 185 48 ## [15] 3 ## ## $density ## [1] 0.003406575 0.085164367 0.228240504 0.487140181 1.018565832 ## [6] 1.645375575 2.476579799 3.134048714 3.406574689 3.004598876 ## [11] 2.319877363 1.386475898 0.630216317 0.163515585 0.010219724 ## ## $mids ## [1] 0.225 0.275 0.325 0.375 0.425 0.475 0.525 0.575 0.625 0.675 0.725 ## [12] 0.775 0.825 0.875 0.925 ## ## $xname ## [1] &quot;posteriorB&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; ## [1] 5871 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.2237 0.5323 0.6127 0.6091 0.6913 0.9103 It is time to compare these two aproximations: par(mfrow = c(2,1)) hist(signupsA, xlim = c(0, 100), col = &quot;pink&quot;, main = &quot;Histograms of Signups Projected in 100 Potential Clients With Strategy A&quot;) hist(signupsB, xlim = c(0, 100), col = &quot;dodgerblue4&quot;, main = &quot;Histograms of Signups Projected in 100 Potential Clients With Strategy B&quot;) It appears that Strategy B is more likely to yield higher signups. In order to check if it is so we will generate a table of rate differences: Where the variable rate_diff is calculated as follows, since we are interested in variable B: \\[rate\\_diff = \\text{rateB}-\\text{rateA}\\] The next table is a representation of the first values obtained: ## PostA PostB rate_diff ## 1 0.4145463 0.4686832 0.05413691 ## 2 0.2659726 0.6099820 0.34400938 ## 3 0.4348927 0.6927303 0.25783757 ## 4 0.3694889 0.6500233 0.28053442 ## 5 0.4394315 0.4623911 0.02295960 ## 6 0.3117022 0.7195024 0.40780021 A histogram of the values recorded in rate_diff is usefull to visualize the probability of strategy B yelding more signups than strategy A. It becomes obvious that the greater part of the probability is beyond 0, meaning that strategy B is more likely to bring mor signups to the company. If the CEO wanted to know how much likely the probability of strategy B is to be sucessful (more signups) than strategy A, this could be computated as well with the followin lines of code: sum((rate$rate_diff &gt; 0)/length(rate$rate_diff)) ## [1] 0.9143246 Strategy B is 91.4% more likely to bring more signups than strategy A. "],
["taller-r.html", "Capítulo 8 Taller R 8.1 ¿Cómo instalar las herramientas? 8.2 ¿Qué es R? 8.3 ¿Cómo obtener ayuda? 8.4 Entrada de datos y su evaluación en la consola 8.5 Estructuras de datos en R 8.6 Utilizando R y RStudio 8.7 ¿Cómo compartir datos y análisis con estadísticos o científicos de datos?", " Capítulo 8 Taller R 8.1 ¿Cómo instalar las herramientas? 8.1.1 Instalando R Las herramientas que utilizaremos será la última versión liberada del software (al momento de hacer éste taller la versión actual es R 3.4.1): Para usuarios Windows. Para usuarios MacOS Para usuarios Linux: Debiant Suse Ubuntu 8.1.2 Instalando RStudio (Integrated Development Environment para R) El utilizar un IDE simplifica considerablemente el trabajo a realizar con R. Instalaciones para: Windows MacOS_X Ubuntu_32Bit Ubuntu_64Bit Fedora 19+/Redhat7+_32Bit Fedora 19+/Redhat7+_64Bit 8.2 ¿Qué es R? 8.2.1 Historia R fué desarrollado a partir de S, un lenguaje desarrollado en Bell Labs por John Chambers y colaboradores. S inicialmente se corría con librerías de Fortran. En 1988 S fué reescrito en C, un lenguaje que también fué desarrollado en Bell Labs. En el libro Statistical Models in S (conocido como el white book) se documentan la funcionalidad para análsis estadísticos a éste nivel de desarrollo de S. La versión 4 de S se liberó en 1998 y se parece mucho a los softwares actuales derivados de S: R y S-PLUS. El libro Programing with Data (conocido como el green book) por John Chambers documenta ésta versión del lenguaje. La filosofía de S: “Queríamos que los usuarios pudieran empezar en un ambiente interactivo, donde ellos no se pensaran, de forma conciente, como programadores. Posteriormente, mientras sus necesidades se hacían más claras y su sofisticación aumentara, podrían”deslizarse&quot; hacia la programación, cuando el lenguaje y el sistema como realizara los procesos se volvieran más importantes para ellos.&quot; (Chambers 2000) R fué desarrollado a partir de S en Nueva Zelanda por Ross Ihaka y Robert Gentleman. Su experiencia en el proceso está documentada en un paper publcado por la Universidad de Auckland, NZ. En 1993 se libera por primera vez al público. En 1997 se forma el R Core Group con gente asociada a S-PLUS. Éste grupo controla el código fuente de R. En 2000 se libera R version 1.0.0 El 7 de Marzo de 2017 se libera R version 3.3.3 8.2.2 Generalidades La sintaxis de R es muy similar a S mas no su semántica. El software es ligero, la funcionalidad del mismo está dada en un formato modular, a través de paquetes. Las capacidades gráficas son sofisticadas y mejores que la mayoría de los paquetes estadísticos. Funciona para realizar actividades de interacción pero también contiene un poderoso lenguaje de programación para el desarrollo de nuevas herramientas (usuario-&gt;programador). La comunidad R es muy activa y dinámica. 8.3 ¿Cómo obtener ayuda? Como se ha comentado en las secciones anteriores, la comunidad R es muy activa, uno de los lugares donde usualmente se obtienen buenas orientaciones es en Stack Overflow. Otro lugar donde se pueden obtener información sobre una función o paquete en específico es en la documentación del paquete que se instala al descargar el mismo. En el siguiente ejemplo se puede ver cómo se explora el comando plot de los paquetes base de R: ?plot Una vez que entiendo cómo trabaja la función plot entonces puedo probar con datos y ver qué obtengo. Ésta es una manera frecuente de resolver dudas o problemas de código. plot(cars) Otro buen lugar para obtener información son las cheat sheets, publicadas por RStudio en su mayoría. Un recurso muy utilizado es la búsqueda en Google. Los cursos en línea han tenido un significativo auge en los últimos años, comparto algunos que considero valen la pena, ésta lista no es exhaustiva: DataCamp DataQuest Udacity Udemy MOOCs: Coursera Edx 8.4 Entrada de datos y su evaluación en la consola Una vez que abrimos la consola R podemos empezar a escribir en el R prompt, los caracteres que se ingresan al R prompt se le llaman expresiones, hay algunas reglas básicas para entrar expresiones: &lt;- es el símbolo que se utiliza como operador de asignación, también es posible utilizar =. #: R interpreta todo lo que esté a la derecha de éste símbolo como un comentario y no lo toma en cuenta para cálculos. Ejemplos: x &lt;- 1 #No se imprime nada, solo se creó un nuevo objeto con un valor numérico. print(x) #impresión explícita ## [1] 1 msg &lt;- &quot;hello&quot; #No se imprime nada, se creó el objeto `msg` con valores `string` o `character` msg #auto-impresión ## [1] &quot;hello&quot; x #auto-imrpesión ## [1] 1 8.5 Estructuras de datos en R 8.5.1 Clases básicas de objetos R tiene 5 clases básicas o atómicas de objetos: Character Valores cualitativos con escalas nominales u ordinales. Numeric (números reales) Números con valores cuantitativos con escalas continuas. Entre un valor y otro hay infinito de otros valores. Integer Números completos, sin fracciones, pueden ser positivos o negativos. Conocidos como valores cuantitativos con escalas discretas. Equidistancia entre un valor y otro. Complex Logical (booleans/binarios/falso-verdadero) Valores con solo dos opciones posibles, ambas mutuamente exclusivas y opuestas. El objeto más básico es el vector: Un vector solo puede contener objetos de la misma clase. La lista permite contener objetos de distintos tipos como un vector. 8.5.2 Atributos Se puede accceder a los atributos de un objeto en R a través de la función attributes(). La información que R entregue será la siguiente: names/dimnames Nombres y/o nombres de dimensiones. dimensions En el caso de matrices y arreglos. class Clase del objeto. length Longitud del objeto. Otros atributos y metadata definidos por el usuario. 8.6 Utilizando R y RStudio En esta sección nos apoyamos con las ilustraciones de Carlos Pérez González y Marcos Colabrook Santamaría (Carlos Pérez-González 2014) 8.6.1 Componentes RStudio Lo primero que haremos es abrir RStudio y explorar los componentes básicos: 8.6.2 Iniciando un proyecto 8.6.2.1 Paso 1 En la esquina superior derecha, identifica el ícono de projecto y da click en él. 8.6.2.2 Paso 2 Puedes elegir crear el proyecto en un directorio que ya exista o crear un directorio nuevo para iniciar. Aún puedes conectar tu projecto a algún administrador de versiones (Git/GitHub). 8.6.2.3 Paso 3 Por ahora la opción empty project es la que utilizaremos. 8.6.2.4 Paso 4 Selecciona el nombre del directorio así como el folder donde se creará en tu ordenador. 8.6.2.5 Paso 5 Al completar los pasos, tu sesión de RStudio debería verse como se muestra en la imagen. 8.6.3 Cargando los datos Hay muchas maneras de cargar datos a R, se puede hacer con líneas de código desde la consola, a través de paquetes preinstalados que ayudan a leer datos en formatos específicos o através de los íconos de RStudio. La siguiente imagen muestra cómo realizar el proceso a través de RStudio: Para éste ejercicio utilizaremos una base de datos que se puede obtener aquí provista amablemente por la compañera Pamela. Si realizamos el ejercicio desde RStuio se producirá el siguiente código con el siguiente resultado. Toma en cuenta que la dirección cambiará dependiendo dónde haz guardado el archivo. library(readr) example &lt;- read_csv(&quot;D:/Dropbox/MsC UABC/2o Semestre/Clases/Estadistica/estadistica-syllabus/datasets/example.csv&quot;) head(example) ## # A tibble: 6 x 17 ## X1 `A\\xd1O` EST ENT ENERO FEBRERO MARZO ABRIL MAYO JUNIO ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA ## 3 1 1965 4 7 18.83 10.02 36.22 102.36 159.6 149.47 ## 4 2 1966 4 7 22.00 1.00 0.00 98.00 197.0 141.20 ## 5 3 1967 4 7 6.00 40.00 0.00 41.00 137.0 215.00 ## 6 4 1968 2 1 20.40 4.60 45.00 100.00 153.8 165.60 ## # ... with 7 more variables: JULIO &lt;dbl&gt;, AGOSTO &lt;dbl&gt;, SEPTIEMBRE &lt;dbl&gt;, ## # OCTUBRE &lt;dbl&gt;, NOVIEMBRE &lt;int&gt;, DICIEMBRE &lt;int&gt;, VR &lt;int&gt; 8.6.4 Preparando la base de datos De entrada vemos que la base de datos podría “limpiarse” si se eliminaran las filas 1 y 2 así como la columna 1 ya que no aportan información relevante. example &lt;- example[-1:-2,-1] #Estamos creando un subconjunto de `dataset` que se compone de todos los elementos de la base de datos menos las filas 1 y 2 así como la columna 1. colnames(example)[1]&lt;-&quot;Año&quot; #Arregla el problema de caracter en la variable &quot;Año&quot; head(example) ## # A tibble: 6 x 16 ## Año EST ENT ENERO FEBRERO MARZO ABRIL MAYO JUNIO JULIO ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1965 4 7 18.83 10.02 36.22 102.36 159.6 149.47 154.37 ## 2 1966 4 7 22.00 1.00 0.00 98.00 197.0 141.20 158.00 ## 3 1967 4 7 6.00 40.00 0.00 41.00 137.0 215.00 110.00 ## 4 1968 2 1 20.40 4.60 45.00 100.00 153.8 165.60 156.40 ## 5 1969 2 1 5.00 0.00 1.00 222.00 102.0 191.00 124.00 ## 6 1970 2 1 140.00 3.00 105.00 116.00 176.0 302.00 158.00 ## # ... with 6 more variables: AGOSTO &lt;dbl&gt;, SEPTIEMBRE &lt;dbl&gt;, ## # OCTUBRE &lt;dbl&gt;, NOVIEMBRE &lt;int&gt;, DICIEMBRE &lt;int&gt;, VR &lt;int&gt; Con respecto a datos faltantes podemos ver que en la última fila, correspondiente al año 2015, los valores correspondientes al mes de Noviembre y Diciembre están faltantes. Para solucionar éste debemos implementar algún método de imputación de datos para calcular los valores faltantes. El proceso a seguir depende de cada área del conocimiento. 8.6.5 Visualización de datos para análisis exploratorios 8.6.5.1 Buscando tipo de distribución en los datos Para poder evaluar de una manera más cómoda y rápida, vamos a generar histogramas de la distribución de los datos contenidos en todas las variables (columnas) de nuestra base de datos, en éste caso necesitariemos hacer 12 histogramas y compararlos juntos. Utilizaremos el paquete lattice para ésta tarea en específico. Los paquetes se instalan con la funcion install.packages() dentro de los parentesis se pone el nombre del paquete entre comillas. Una vez instalado el paquete se debe llamar a la consola con la función library() y el nombre del paquete dentro de los paréntesis sin comillas. #install.packages(&quot;lattice&quot;) library(lattice) histogram( ~ ENERO +FEBRERO +MARZO +ABRIL +MAYO +JUNIO +JULIO +AGOSTO +SEPTIEMBRE +OCTUBRE +NOVIEMBRE +DICIEMBRE, data = example, main = &#39;Histogramas para Evaluación Inicial de Distribución de los Datos&#39;) Ésta información se puede obtener también con la función summary(): summary(example) ## Año EST ENT ENERO ## Min. :1965 Min. :1.000 Min. :1.000 Min. : 0.0 ## 1st Qu.:1978 1st Qu.:1.000 1st Qu.:1.000 1st Qu.: 0.0 ## Median :1990 Median :1.000 Median :1.000 Median : 7.0 ## Mean :1990 Mean :1.549 Mean :1.353 Mean : 18.4 ## 3rd Qu.:2002 3rd Qu.:2.000 3rd Qu.:1.000 3rd Qu.: 21.2 ## Max. :2015 Max. :4.000 Max. :7.000 Max. :140.0 ## ## FEBRERO MARZO ABRIL MAYO ## Min. : 0.0 Min. : 0.00 Min. : 0.0 Min. : 55.0 ## 1st Qu.: 0.0 1st Qu.: 0.00 1st Qu.: 50.0 1st Qu.:136.5 ## Median : 2.0 Median : 28.80 Median : 88.0 Median :167.0 ## Mean : 10.5 Mean : 36.46 Mean :107.5 Mean :169.8 ## 3rd Qu.: 12.5 3rd Qu.: 51.00 3rd Qu.:139.0 3rd Qu.:197.5 ## Max. :113.0 Max. :132.00 Max. :465.0 Max. :367.0 ## ## JUNIO JULIO AGOSTO SEPTIEMBRE ## Min. : 16.0 Min. : 60.0 Min. : 39.0 Min. : 30.1 ## 1st Qu.:111.0 1st Qu.:110.0 1st Qu.:141.5 1st Qu.:123.0 ## Median :143.0 Median :156.4 Median :171.0 Median :166.5 ## Mean :155.7 Mean :162.8 Mean :179.1 Mean :171.3 ## 3rd Qu.:189.5 3rd Qu.:198.1 3rd Qu.:212.0 3rd Qu.:217.0 ## Max. :388.0 Max. :311.0 Max. :402.0 Max. :455.0 ## ## OCTUBRE NOVIEMBRE DICIEMBRE VR ## Min. : 32.0 Min. : 0.00 Min. : 0.00 Min. : 396 ## 1st Qu.:104.5 1st Qu.: 66.75 1st Qu.: 12.50 1st Qu.:1145 ## Median :132.8 Median :105.50 Median : 29.00 Median :1297 ## Mean :152.2 Mean :117.68 Mean : 46.28 Mean :1295 ## 3rd Qu.:185.0 3rd Qu.:155.75 3rd Qu.: 64.75 3rd Qu.:1466 ## Max. :289.0 Max. :374.00 Max. :208.00 Max. :1957 ## NA&#39;s :1 NA&#39;s :1 De los resultados generados por summary() se pueden obtener la siguiente información de cada variable: Media Mediana Rango Intercuartil Valor mínimo Valor máximo 8.6.5.2 Medidas de tendencia central Seleccionaremos los meses de Abril y Agosto para calcular medidas de tendencia central, de los histogramas previos podemos intuir que uno de éstos meses tiene un “comportamiento normal” y el otro no. Para éste ejercicio utilizarmos el generador de gráficas que viene con los paquetes báiscos cuando se instala R. 8.6.5.2.1 Distribuciones no Normales Gráfica Base hist(example$ABRIL, #Histograma base col = &quot;darksalmon&quot;, #color del relleno de las barras border = &quot;black&quot;, #color del borde de las barras prob = TRUE, #le indica a R que los histogramas se realizaran en base a densidad y no a cantidad. xlab = &quot;Milímetros de Agua&quot;, ylab = &quot;Frecuencia&quot;, main = &quot;Precipitación durante el mes de Abril de los años 1965 a 2015&quot;) Para una lista de los colores que puedes utilizar con las gráficas base de R puedes consultar aquí. Medidas de Tendencia Central Una vez que la gráfica base está desarrollada, procederemos a agregar la curva de distribución, las líneas de media y mediana. hist(example$ABRIL, #Histograma base col = &quot;darksalmon&quot;, #color del relleno de las barras border = &quot;black&quot;, #color del borde de las barras prob = TRUE, #le indica a R que los histogramas se realizaran en base a densidad y no a cantidad. xlab = &quot;Milímetros de Agua&quot;, ylab = &quot;Frecuencia&quot;, main = &quot;Precipitación durante el mes de Abril de los años 1965 a 2015&quot;) ##Distribución## lines(density(example$ABRIL), #Línea base de distribución lwd = 3, #ancho de la linea col = &quot;deeppink&quot; #color of line ) ##Media## abline(v = mean(example$ABRIL), #Línea base lwd = 2, #Ancho de línea col = &quot;blue&quot; #Color de línea ) ##Mediana## abline(v = median(example$ABRIL), ##Línea básica lwd = 2, col = &quot;chartreuse&quot;) Leyenda hist(example$ABRIL, #Histograma base col = &quot;darksalmon&quot;, #color del relleno de las barras border = &quot;black&quot;, #color del borde de las barras prob = TRUE, #le indica a R que los histogramas se realizaran en base a densidad y no a cantidad. xlab = &quot;Milímetros de Agua&quot;, ylab = &quot;Frecuencia&quot;, main = &quot;Precipitación durante el mes de Abril de los años 1965 a 2015&quot;) ##Distribución## lines(density(example$ABRIL), #Línea base de distribución lwd = 3, #ancho de la linea col = &quot;deeppink&quot; #color of line ) ##Media## abline(v = mean(example$ABRIL), #Línea base lwd = 2, #Ancho de línea col = &quot;blue&quot; #Color de línea ) ##Mediana## abline(v = median(example$ABRIL), ##Línea básica lwd = 2, col = &quot;chartreuse&quot;) ##Leyenda legend(x = &quot;topright&quot;, #Dónde se va a ubicar la leyenda con relación a la gráfica c(&quot;Gráfica de Densidad&quot;, &quot;Media&quot;, &quot;Mediana&quot;), lwd = c(3,2,2), col = c(&quot;deeppink&quot;, &quot;blue&quot;, &quot;chartreuse&quot;)) 8.6.5.3 Distribuciones Normales AGOSTO Realiza el mismo proceso para obtener una gráfica del mes de AGOSTO como la que se presenta abajo: 8.7 ¿Cómo compartir datos y análisis con estadísticos o científicos de datos? Una breve guía de cómo compartir datos con personas que se dedican a la estadística o ciencia de datos (Leek 2016), Jeff Leek dice que los datos compartidos deben venir ordenados en cuatro grandes grupos/folders/repositorios: Datos crudos. Datos que no han sido manipulados por ningún software de minería de datos o algún otro. Una base de datos “arreglada” (tidy dataset). Cada fila una instancia/paciente/ente a medir. Cada columna una variable. Cada celda un valor de la variable correspondiente. Un libro de códigos que explique cada variable y sus valores en en la base de datos “arreglada”. Una receta explícita y exacta de lo que se hizo para ir de 1 –&gt; 2,3, étc References "],
["tareas.html", "Capítulo 9 Tareas 9.1 Point Chart 9.2 Stratified Sampling", " Capítulo 9 Tareas 9.1 Point Chart 9.2 Stratified Sampling author: Dolores Ojeda, Gener Avilés R date: 2017-03-05 9.2.1 What is Stratified Sampling? Population is partitioned in non-overlaping groups, called strata and a sample is collected from each stratum following a determined design. 9.2.2 Why use Stratified Sampling? - May produce smaller error when estimating than simple random sample. Specially when measurements within strata have realitve small variation. - Cost by observation reduced. - There may be a need to have a subgroup (stratum) with similar estimates of those of the population. 9.2.3 Example The Titanic Database: ## pclass survived name sex ## Min. :1.000 Min. :0.000 Length:1310 Length:1310 ## 1st Qu.:2.000 1st Qu.:0.000 Class :character Class :character ## Median :3.000 Median :0.000 Mode :character Mode :character ## Mean :2.295 Mean :0.382 ## 3rd Qu.:3.000 3rd Qu.:1.000 ## Max. :3.000 Max. :1.000 ## NA&#39;s :1 NA&#39;s :1 ## age sibsp parch ticket ## Min. : 0.1667 Min. :0.0000 Min. :0.000 Length:1310 ## 1st Qu.:21.0000 1st Qu.:0.0000 1st Qu.:0.000 Class :character ## Median :28.0000 Median :0.0000 Median :0.000 Mode :character ## Mean :29.8811 Mean :0.4989 Mean :0.385 ## 3rd Qu.:39.0000 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :80.0000 Max. :8.0000 Max. :9.000 ## NA&#39;s :264 NA&#39;s :1 NA&#39;s :1 ## fare cabin embarked ## Min. : 0.000 Length:1310 Length:1310 ## 1st Qu.: 7.896 Class :character Class :character ## Median : 14.454 Mode :character Mode :character ## Mean : 33.295 ## 3rd Qu.: 31.275 ## Max. :512.329 ## NA&#39;s :2 ## boat body home.dest ## Length:1310 Min. : 1.0 Length:1310 ## Class :character 1st Qu.: 72.0 Class :character ## Mode :character Median :155.0 Mode :character ## Mean :160.8 ## 3rd Qu.:256.0 ## Max. :328.0 ## NA&#39;s :1189 9.2.3.1 Variable Codes - Pclass: 1 = Upper, 2 = Middle, 3 = Lower. - SibSp: Number of Siblings/Spouses aboard. - Parch: Number of Parents/Children Aboard. - Embarked: C = Cherbourg, Q = Queenstown, S = Southampton. 9.2.3.2 Calculating Probabilities to select people who embarked in Queenstown \\(P(A) = \\frac{\\text{Numero de elementos de A}}{n}\\) There are 1310 entries, and 123 of them embarked in Queenstown, nevertheless the risk of dying was equally present for them as for the passengers from Southampton or Cherbourg. If a uniform proability is calculated the numbers are: - \\(P(Q) = \\frac{123}{1310} =\\) 0.0938931 - \\(P(C) = \\frac{270}{1310} =\\) 0.2061069 - \\(P(S)\\frac{914}{1310} =\\) 0.6977099 This approximation will hinder the process of data mining and, eventually, the generation of a machine learning model for survival prediction. 9.2.3.3 Fixing the Problem By using stratified sampling we can raise the probability for the group that boarded in Queenstown and survived to be selected, therefore, taken in consideration for the generation of a survival prediction model. For this we will use conditional probability: \\(P(Survived|EmbarkedQ)= \\frac{P(Survived\\cap EmbarkedQ)}{P(EmbarkedQ)} = \\frac{44}{123}=\\) 0.3577236 library(dplyr) Q&lt;-filter(titanic, embarked == &quot;Q&quot; &amp; survived == 1) count(Q) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 44 "]
]
